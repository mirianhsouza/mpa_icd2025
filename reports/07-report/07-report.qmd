---
title: "Introdu√ß√£o √† Ci√™ncia de Dados"
subtitle: "Aula 8"
lang: pt-BR
author: "Seu Nome"
format: 
  html:
    theme: cosmo
    embed-resources: true
    toc: true
    number-sections: true
execute:
  message: false
  warning: false
  echo: true
---

<style>
body {
  font-size: 13pt;
  text-align: justify;
}
</style>

```{r}
#| label: setup
#| echo: false

# configura exibi√ß√£o de n√∫meros
options(digits = 8, scipen = 999)

# carrega pacotes
library(tidyverse)
```





# Conceitos Fundamentais de Probabilidade

## Propriedades da Vari√¢ncia

Propriedades em R

```{r}
# Exemplo das propriedades
set.seed(123)
investimento <- rnorm(1000, mean = 0.05, sd = 0.12)  # Retorno 5%, vol 12%
```


```{r}
# Propriedade 1: V(X + constante) = V(X)
# Adicionar uma constante n√£o altera o risco
taxa <- 0.02
var(investimento)
var(investimento + taxa)  # Vari√¢ncia igual
```


```{r}
# Propriedade 2: V(a*X) = a¬≤*V(X)  
# Dobrar o investimento quadruplica o risco
dobro_investimento <- 2 * investimento
var(investimento)
var(dobro_investimento)  # 4 vezes maior
2^2 * var(investimento)  # Confirma√ß√£o te√≥rica
```



## Como simular uma VA com distribui√ß√£o de Bernoulli


```{r}
# Fixa a semente para reprodutibilidade
set.seed(123) 

# Simula 10 observa√ß√µes de uma VA Bernoulli
simulacao_bernoulli <- rbinom(n = 10, size = 1, prob = 0.5)
simulacao_bernoulli
```




## Como simular uma VA com distribui√ß√£o Binomial?


```{r}
# Fixa a semente para reprodutibilidade
set.seed(123) 

# Simula 100 observa√ß√µes de uma VA Binomial
simulacao_binomial <- rbinom(n = 100, size = 10, prob = 0.5)
simulacao_binomial
```




# Distribui√ß√µes de Probabilidade de VA Cont√≠nuas 


## Distribui√ß√£o Uniforme Cont√≠nua


Exemplo em R:

Gerando 10 n√∫meros uniformes no intervalo [0,1]:

```{r}
set.seed(42)  # para reprodutibilidade
runif(10)
```

Visualizando a distribui√ß√£o com um histograma:

```{r}
amostra <- runif(10000)
hist(amostra, breaks = 20, col = "steelblue", main = "Distribui√ß√£o Uniforme U(0,1)",
     xlab = "Amostra gerada", probability = TRUE)
```

Aproximando $P(0.2 < Y < 0.5)$:

```{r}
mean(amostra > 0.2 & amostra < 0.5)  # Aproxima√ß√£o emp√≠rica de P(0.2 < Y < 0.5)
```

Valor exato: $P(0.2 < Y < 0.5) = 0.5 - 0.2 = 0.3$


Aplica√ß√µes

* Base para **simula√ß√µes de Monte Carlo**
* Constru√ß√£o de vari√°veis aleat√≥rias de qualquer outra distribui√ß√£o
* Gera√ß√£o de amostras aleat√≥rias na pr√°tica




## Transforma√ß√µes da Uniforme


Exemplo 1: Gerando vari√°vel Exponencial a partir de $U(0,1)$

Queremos gerar uma vari√°vel $X \sim \text{Exponencial}(\lambda)$, 
cuja **fun√ß√£o de distribui√ß√£o acumulada (CDF)** √©:

$$
F_X(x) = 1 - e^{-\lambda x}, \quad x \geq 0
$$

O m√©todo da **inversa da CDF** nos diz que podemos gerar amostras 
de $X$ se:

$$
X = F_X^{-1}(U), \quad \text{com } U \sim \mathcal{U}(0,1)
$$

üìê Invertendo a CDF:

Come√ßamos com:

$$
u = F_X(x) = 1 - e^{-\lambda x}
$$

Resolvendo para $x$:

$$
e^{-\lambda x} = 1 - u
\Rightarrow -\lambda x = \ln(1 - u)
\Rightarrow x = -\frac{1}{\lambda} \ln(1 - u)
$$


Mas por que usamos $\ln(U)$ em vez de $\ln(1 - U)$?

Como $U \sim U(0,1)$, ent√£o tamb√©m $1 - U \sim U(0,1)$. A distribui√ß√£o 
√© **sim√©trica** em torno de 0.5, ent√£o qualquer transforma√ß√£o baseada 
em uma vari√°vel uniforme tem a **mesma distribui√ß√£o** se feita sobre 
$U$ ou $1 - U$.

Portanto, por **conveni√™ncia computacional**, usamos:

$$
X = -\frac{1}{\lambda} \ln(U)
$$

Esse valor segue a mesma distribui√ß√£o exponencial.

Em R:

```{r}
# Gerando 10.000 valores da Exponencial(Œª) com Œª = 2
set.seed(42)
u <- runif(10000)      # Uniforme(0,1)
lambda <- 2
x <- -log(u) / lambda  # Exponencial(Œª)

# Visualizando a distribui√ß√£o
hist(x, breaks = 30, col = "orange", probability = TRUE,
     main = "Exponencial(Œª = 2) gerada a partir de U(0,1)",
     xlab = "x")

# Comparando com densidade te√≥rica
curve(dexp(x, rate = lambda), add = TRUE, col = "blue", lwd = 2)
legend("topright", legend = c("Histograma simulado", "Densidade te√≥rica"),
       fill = c("orange", NA), border = NA, lty = c(NA, 1), col = c(NA, "blue"))
```


Exemplo 2: Gera√ß√£o de Normal Padr√£o

N√£o existe inversa anal√≠tica simples para a CDF da dist. normal padronizada
$\Phi(x)$, mas:

- Pode-se usar aproxima√ß√µes num√©ricas (fun√ß√£o `qnorm()` em R):

```{r}
z <- qnorm(runif(10000))  # Z ~ N(0,1)
hist(z, breaks = 30, col = "skyblue", probability = TRUE,
     main = "Normal padr√£o gerada com qnorm(runif())")
curve(dnorm(x), add = TRUE, col = "darkblue", lwd = 2)
```




# Distribui√ß√£o Normal 


## Fun√ß√µes R para Distribui√ß√£o Normal 


Exemplos:

```{r}
#| echo: true
# Simular 5 valores de uma distr. normal padr√£o
set.seed(123)
rnorm(5)
```

```{r}
# f(0) - densidade no ponto 0
dnorm(0)
```

```{r}
# P(Z ‚â§ 1.96)
pnorm(1.96)
```

```{r}
# Encontra z tal que P(Z ‚â§ z) = 0.975
qnorm(0.975)
```



## Exerc√≠cio 1 

Calcule as seguintes probabilidades associadas a uma distribui√ß√£o normal padronizada.

a. $P(Z <= 1.25)$

```{r}
pnorm(1.25)
```


b. $P(Z > 1.25)$

```{r}
pnorm (1-(1.25))
```

c. $P(Z <= - 1.25)$

```{r}
pnorm (- 1.25)
```

d. $P(-0.8 <= Z <= 1.25)$

```{r}
pnorm (-0.8 <= 0,8 <= 1.25)
```


## Exerc√≠cio 2 

::: {.callout-note icon="false"}
## Distribuicao Normal: Calcule os quantis associados √†s seguintes probabilidades

a. $P(Z < q)$ = 0.9798

```{r}
qnorm (0.9798)
```

b. $P(Z < q)$ = 0.2546

```{r}
qnorm (0.2546)
```

c. $P(Z > q)$ = 0.1075

```{r}
qnorm (1- 0.1075)
```

d. $P(Z > q)$ = 0.9418
:::
```{r}
qnorm (1-0.9418)
```




# Simula√ß√£o de Monte Carlo {.center background-color="#F1E9D2"}


## M√©todos de Monte Carlo

**Exemplo 1:** Probabilidade de obter "cara" em lan√ßamentos de uma moeda 
honesta

Vamos simular $n$ lan√ßamentos e estimar $P(\text{cara})$:

```{r}
mean(sample(0:1, 10, replace = TRUE)) # 10 lan√ßamentos
```

```{r}
mean(sample(0:1, 100, replace = TRUE)) # 100 lan√ßamentos
```

```{r}
mean(sample(0:1, 1000, replace = TRUE)) # 1000 lan√ßamentos 
```

```{r}
mean(sample(0:1, 10000, replace = TRUE)) # 10000 lan√ßamentos 
```

- `0` representa "coroa" e `1` representa "cara".

- A fun√ß√£o `mean()` fornece a propor√ß√£o de caras, que √© uma estimativa de 
$P(\text{cara}) = 0.5$.



**Exemplo 2** Probabilidade de obter tr√™s "caras" em tr√™s lan√ßamentos

```{r}
n <- 10000
vetor_simulacoes <- numeric(n)

for (i in 1:n) {
  teste <- sample(0:1, 3, replace = TRUE)
  vetor_simulacoes[i] <- as.numeric(sum(teste) == 3)
}

mean(vetor_simulacoes)
```

* Cada itera√ß√£o simula 3 lan√ßamentos.
* Se todos os 3 forem "cara", soma ser√° 3, registrando um "sucesso".
* A m√©dia final √© a estimativa de $P(\text{3 caras}) = \frac{1}{8} = 0,125$.

**Conclus√£o**

M√©todos de Monte Carlo baseiam-se na ideia de que, ao repetir um experimento 
aleat√≥rio muitas vezes, a m√©dia dos resultados converge para o valor 
esperado do fen√¥meno modelado ‚Äî uma consequ√™ncia direta da 
**Lei dos Grandes N√∫meros**. Isso os torna ferramentas poderosas para 
resolver problemas onde abordagens determin√≠sticas s√£o impratic√°veis.




## Integra√ß√£o de Monte Carlo


**Exemplo 1**

Para resolver a seguinte integral definida via simula√ß√£o de Monte Carlo:

$$
\int_{2}^{5} \sin(x)dx
$$

basta fazermos:

```{r}
# fixando a semente do algoritmo
set.seed(2023)

# integracao via SMC

## Gerando amostras uniformes no intervalo [2, 5]
u <- runif(100000, min = 2, max = 5)

## Calculando a m√©dia/valor esperado da fun√ß√£o seno
(5 - 2)*mean(sin(u))
```

Resolvendo analiticamente a integral, verifica-se que o valor 
exato √© -0,7.

```{r}
# resolvendo numericamente a integral
integrate(sin, 2, 5)
```


**Exemplo 2:**

Considere a integral:

$$
\int_{0}^{1} \frac{\sin(x(1 - x))}{1 + x + \sqrt{x}} \, dx
$$

Esta integral pode ser estimada por Monte Carlo amostrando 
$U_i \sim \mathcal{U}(0,1)$ e calculando a m√©dia:

```{r}
# Semente para reprodutibilidade
set.seed(2023)

# Fun√ß√£o de interesse
f <- function(x) sin(x*(1 - x)) / (1 + x + sqrt(x))

# Integra√ß√£o via Monte Carlo
u <- runif(100000)
mean(f(u))  # Estimativa da integral
```

Podemos comparar com o valor exato (num√©rico) da integral usando o 
m√©todo adaptativo de quadratura:

```{r}
integrate(f, 0, 1)
```

**Conclus√£o**

* A **integra√ß√£o de Monte Carlo** transforma um problema determin√≠stico 
  (uma integral) em um problema probabil√≠stico (c√°lculo de um 
  valor esperado).
  
* Os **limites de integra√ß√£o** determinam o intervalo da distribui√ß√£o 
  uniforme usada para gerar amostras.
  
* A precis√£o aumenta com o n√∫mero de amostras, gra√ßas √† **Lei dos Grandes N√∫meros**.





## Exerc√≠cio 1

Simule 1.000 realiza√ß√µes de uma vari√°vel aleat√≥ria ($Z$) que segue uma 
distribui√ß√£o normal padronizada ($Z \sim N(\mu = 0, \sigma = 1)$) e use 
sua amostra simulada para estimar as seguintes probabilidades:

a. $P(Z > 2.5)$

```{r}
z <- rnorm(1000)

sum(z > 2.5) / 1000

1 - pnorm(2.5)
```
b. $P(-1.2 < Z < 1.3)$

```{r}
set.seed(123) 

z <- rnorm(1000)

mean(z > -1.2 & z < 1.3)

pnorm(1.3) - pnorm(-1.2)
```


Compare os dois resultados com o valor exato que pode ser obtido com 
a fun√ß√£o `pnorm()`.





## Exerc√≠cio 2


Use Simula√ß√£o de Monte Carlo para estimar as seguintes integrais utilizando 
1000 repeti√ß√µes. Compare com a resposta exata, a qual pode ser obtida com a 
fun√ß√£o `integrate()` da linguagem R.

a. $\int_{1}^{3} x^2dx$.

```{r}
set.seed()  
n <- 100000

u <- runif(n, min = 1, max = 3)

estimativa_mc <- (3 - 1) * mean(u^2)
estimativa_mc
```
```{r}
f1 <- function (x) {out <-x^2}
integrate(f1, lower = 1, upper=3)
```
```


b. $\int_{0}^{\pi} sin(x)dx$

```{r}
```{r}
set.seed(789) 

b <- 100000
u <- runif(n, min = 0, max = pi)
mean(sin(b))*pi
```
```





